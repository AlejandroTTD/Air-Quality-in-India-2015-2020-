{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "3f31a98e",
            "metadata": {},
            "source": [
                "<h3 style=\"text-align: left; color: red;\">Librerías importadas</h3>\n",
                "\n",
                "A continuación, se describe el uso de cada librería en nuestro proyecto:\n",
                "- pandas y numpy: Se usan para el manejo óptimo de los archivos \"csv\", ya que manejarlos en su mismo formato lo vuelve complejo de forma innecesaria.\n",
                "- matplotlib y seaborn: Se usan para la obtencion de diagramas y gráficas de los datos.\n",
                "- sklearn: Se usó esta librería contiene todos los métodos de imputación a usarse previo al modelado.\n",
                "- os, shutil y glob: Se usaron estas librerías para el correcto manejo y organización de los archivos obtenidos.\n",
                "- Parallel y delayed: Se usaron estas librerías para el procesamiento en paralelo de los archivos csv, ya que procesarlos de forma secuencial requiere una gran cantidad de recursos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 173,
            "id": "ad5efb20",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.impute import KNNImputer, SimpleImputer\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from joblib import Parallel, delayed\n",
                "import os\n",
                "import shutil\n",
                "import glob"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f50c0f9d",
            "metadata": {},
            "source": [
                "<h3 style=\"text-align: left; color: red;\">Lectura de csv's</h3>\n",
                "\n",
                "Leemos los archivos que se encuentran en la carpeta desde donde se ejecuta el código y los movemos a la carpeta \"Archivos No Imputados\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 174,
            "id": "f2717674",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "city_day.csv ya se encuentra en Archivos No Imputados. Leyendo de nuevo\n",
                        "city_hour.csv ya se encuentra en Archivos No Imputados. Leyendo de nuevo\n",
                        "station_day.csv ya se encuentra en Archivos No Imputados. Leyendo de nuevo\n",
                        "station_hour.csv ya se encuentra en Archivos No Imputados. Leyendo de nuevo\n",
                        "stations.csv ya se encuentra en Archivos No Imputados. Leyendo de nuevo\n"
                    ]
                }
            ],
            "source": [
                "ruta_archivos_csv_origen = os.getcwd() # Obtenemos la ruta desde donde se ejecuta el Notebook\n",
                "ruta_archivos_csv_destino = os.path.join(ruta_archivos_csv_origen, \"Archivos No Imputados\") # Definimos la ruta hacia donde van los archivos csv despues de leerse\n",
                "os.makedirs(\"Archivos No Imputados\", exist_ok = True) # Creamos la carpeta si no existe\n",
                "\n",
                "\n",
                "def leer_mover_archivos (nombre_archivo_csv):\n",
                "    ruta_origen_archivos_csv = os.path.join(ruta_archivos_csv_origen, nombre_archivo_csv) # Definimos la ruta actual del archivo csv\n",
                "    ruta_carpeta_archivos_csv = os.path.join(ruta_archivos_csv_destino, nombre_archivo_csv) # Definimos la ruta donde deberia estar el archivo tras ejecutarse el codigo\n",
                "\n",
                "    # Si el archivo se encuentra en la ruta de origen (desde donde se ejecuta el notebook)\n",
                "    if os.path.exists(ruta_origen_archivos_csv): \n",
                "\n",
                "        archivo_pandas = pd.read_csv(ruta_origen_archivos_csv, low_memory = False) # Leemos el archivo csv\n",
                "        archivo_pandas = archivo_pandas.reset_index(drop = True) \n",
                "        shutil.move(ruta_origen_archivos_csv, ruta_carpeta_archivos_csv) # Movemos el archivo csv desde la ruta de origen hacia la carpeta Archivos No Imputados\n",
                "        print(f\"{nombre_archivo_csv} leido y movido a la carpeta Archivos No Imputados\")\n",
                "        return archivo_pandas\n",
                "    \n",
                "    # Si el archivo se encuentra dentro de la carpeta \"Archivos No Imputados\"\n",
                "    elif os.path.exists(ruta_carpeta_archivos_csv):\n",
                "\n",
                "        archivo_pandas = pd.read_csv(ruta_carpeta_archivos_csv, low_memory = False) # Leemos el archivo csv\n",
                "        archivo_pandas = archivo_pandas.reset_index(drop = True)\n",
                "        print(f\"{nombre_archivo_csv} ya se encuentra en Archivos No Imputados. Leyendo de nuevo\")\n",
                "        return archivo_pandas\n",
                "    \n",
                "    # Si el archivo no se encuentra en ninguna carpeta accesible por nuestro codigo\n",
                "    else: \n",
                "\n",
                "        print(f\"No se encontro el archivo {nombre_archivo_csv}\")\n",
                "        return None\n",
                "       \n",
                "ciudad_dia    = leer_mover_archivos(\"city_day.csv\")\n",
                "ciudad_hora   = leer_mover_archivos(\"city_hour.csv\")\n",
                "estacion_dia  = leer_mover_archivos(\"station_day.csv\")\n",
                "estacion_hora = leer_mover_archivos(\"station_hour.csv\")\n",
                "estaciones    = leer_mover_archivos(\"stations.csv\")\n",
                "\n",
                "\n",
                "# Creamos un diccionario con los nombres de los archivos csv y el nombre con el que se guardaran los archivos \n",
                "# lista = {\n",
                "#     \"ciudad_dia\":\"city_day.csv\",\n",
                "#     \"ciudad_hora\":\"city_hour.csv\",\n",
                "#     \"estacion_dia\":\"station_day.csv\",\n",
                "#     \"estacion_hora\":\"station_hour.csv\",\n",
                "#     \"estaciones\":\"stations.csv\"\n",
                "#     }\n",
                "\n",
                "# class FicheroArchivos:\n",
                "#     listaDataFrames = []\n",
                "#     def __init__(self, listaCSV):\n",
                "        \n",
                "#         for x,y in listaCSV.items():\n",
                "#             self.listaDataFrames.append(leer_mover_archivos(y)) \n",
                "# a = FicheroArchivos(lista)\n",
                "# a.listaDataFrames[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "70beb36a",
            "metadata": {},
            "source": [
                "Para la obtener más claridad acerca de las columnas y el tipo de datos, usamos describe() e info() para filtrar lo que necesitamos usar."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "003149b6",
            "metadata": {},
            "source": [
                "<h3 style=\"text-align: left; color: red;\">Datos por Ciudad-día y filtrado</h3>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 175,
            "id": "a60bd261",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 29531 entries, 0 to 29530\n",
                        "Data columns (total 16 columns):\n",
                        " #   Column      Non-Null Count  Dtype  \n",
                        "---  ------      --------------  -----  \n",
                        " 0   City        29531 non-null  object \n",
                        " 1   Date        29531 non-null  object \n",
                        " 2   PM2.5       24933 non-null  float64\n",
                        " 3   PM10        18391 non-null  float64\n",
                        " 4   NO          25949 non-null  float64\n",
                        " 5   NO2         25946 non-null  float64\n",
                        " 6   NOx         25346 non-null  float64\n",
                        " 7   NH3         19203 non-null  float64\n",
                        " 8   CO          27472 non-null  float64\n",
                        " 9   SO2         25677 non-null  float64\n",
                        " 10  O3          25509 non-null  float64\n",
                        " 11  Benzene     23908 non-null  float64\n",
                        " 12  Toluene     21490 non-null  float64\n",
                        " 13  Xylene      11422 non-null  float64\n",
                        " 14  AQI         24850 non-null  float64\n",
                        " 15  AQI_Bucket  24850 non-null  object \n",
                        "dtypes: float64(13), object(3)\n",
                        "memory usage: 3.6+ MB\n"
                    ]
                }
            ],
            "source": [
                "ciudad_dia.describe()\n",
                "ciudad_dia.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 176,
            "id": "7e71cfab",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 29531 entries, 0 to 29530\n",
                        "Data columns (total 9 columns):\n",
                        " #   Column  Non-Null Count  Dtype  \n",
                        "---  ------  --------------  -----  \n",
                        " 0   City    29531 non-null  object \n",
                        " 1   Date    29531 non-null  object \n",
                        " 2   PM2.5   24933 non-null  float64\n",
                        " 3   PM10    18391 non-null  float64\n",
                        " 4   NO2     25946 non-null  float64\n",
                        " 5   CO      27472 non-null  float64\n",
                        " 6   SO2     25677 non-null  float64\n",
                        " 7   O3      25509 non-null  float64\n",
                        " 8   AQI     24850 non-null  float64\n",
                        "dtypes: float64(7), object(2)\n",
                        "memory usage: 2.0+ MB\n"
                    ]
                }
            ],
            "source": [
                "# Descartamos las variables que no serán usadas, dejando las string para su posterior union\n",
                "ciudad_dia_filtrado_string = ciudad_dia[['City', 'Date', 'PM2.5', 'PM10', 'NO2', 'CO', 'SO2', 'O3', 'AQI']]\n",
                "ciudad_dia_filtrado_string.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5c721450",
            "metadata": {},
            "source": [
                "<h3 style=\"text-align: left; color: red;\">Datos por Ciudad-hora y filtrado</h3>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 177,
            "id": "cf547e97",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 707875 entries, 0 to 707874\n",
                        "Data columns (total 16 columns):\n",
                        " #   Column      Non-Null Count   Dtype  \n",
                        "---  ------      --------------   -----  \n",
                        " 0   City        707875 non-null  object \n",
                        " 1   Datetime    707875 non-null  object \n",
                        " 2   PM2.5       562787 non-null  float64\n",
                        " 3   PM10        411138 non-null  float64\n",
                        " 4   NO          591243 non-null  float64\n",
                        " 5   NO2         590753 non-null  float64\n",
                        " 6   NOx         584651 non-null  float64\n",
                        " 7   NH3         435333 non-null  float64\n",
                        " 8   CO          621358 non-null  float64\n",
                        " 9   SO2         577502 non-null  float64\n",
                        " 10  O3          578667 non-null  float64\n",
                        " 11  Benzene     544229 non-null  float64\n",
                        " 12  Toluene     487268 non-null  float64\n",
                        " 13  Xylene      252046 non-null  float64\n",
                        " 14  AQI         578795 non-null  float64\n",
                        " 15  AQI_Bucket  578795 non-null  object \n",
                        "dtypes: float64(13), object(3)\n",
                        "memory usage: 86.4+ MB\n"
                    ]
                }
            ],
            "source": [
                "ciudad_hora.describe()\n",
                "ciudad_hora.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 178,
            "id": "3469c181",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 707875 entries, 0 to 707874\n",
                        "Data columns (total 9 columns):\n",
                        " #   Column    Non-Null Count   Dtype  \n",
                        "---  ------    --------------   -----  \n",
                        " 0   City      707875 non-null  object \n",
                        " 1   Datetime  707875 non-null  object \n",
                        " 2   PM2.5     562787 non-null  float64\n",
                        " 3   PM10      411138 non-null  float64\n",
                        " 4   NO2       590753 non-null  float64\n",
                        " 5   CO        621358 non-null  float64\n",
                        " 6   SO2       577502 non-null  float64\n",
                        " 7   O3        578667 non-null  float64\n",
                        " 8   AQI       578795 non-null  float64\n",
                        "dtypes: float64(7), object(2)\n",
                        "memory usage: 48.6+ MB\n"
                    ]
                }
            ],
            "source": [
                "# Descartamos las variables que no serán usadas, dejando las string para su posterior union\n",
                "ciudad_hora_filtrado_string = ciudad_hora[['City', 'Datetime', 'PM2.5', 'PM10', 'NO2', 'CO', 'SO2', 'O3', 'AQI']]\n",
                "ciudad_hora_filtrado_string.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4dcf79e4",
            "metadata": {},
            "source": [
                "<h3 style=\"text-align: left; color: red;\">Datos por Estación-día y filtrado</h3>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 179,
            "id": "b061d13d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 108035 entries, 0 to 108034\n",
                        "Data columns (total 16 columns):\n",
                        " #   Column      Non-Null Count   Dtype  \n",
                        "---  ------      --------------   -----  \n",
                        " 0   StationId   108035 non-null  object \n",
                        " 1   Date        108035 non-null  object \n",
                        " 2   PM2.5       86410 non-null   float64\n",
                        " 3   PM10        65329 non-null   float64\n",
                        " 4   NO          90929 non-null   float64\n",
                        " 5   NO2         91488 non-null   float64\n",
                        " 6   NOx         92535 non-null   float64\n",
                        " 7   NH3         59930 non-null   float64\n",
                        " 8   CO          95037 non-null   float64\n",
                        " 9   SO2         82831 non-null   float64\n",
                        " 10  O3          82467 non-null   float64\n",
                        " 11  Benzene     76580 non-null   float64\n",
                        " 12  Toluene     69333 non-null   float64\n",
                        " 13  Xylene      22898 non-null   float64\n",
                        " 14  AQI         87025 non-null   float64\n",
                        " 15  AQI_Bucket  87025 non-null   object \n",
                        "dtypes: float64(13), object(3)\n",
                        "memory usage: 13.2+ MB\n"
                    ]
                }
            ],
            "source": [
                "estacion_dia.describe()\n",
                "estacion_dia.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 180,
            "id": "f2808f28",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 108035 entries, 0 to 108034\n",
                        "Data columns (total 8 columns):\n",
                        " #   Column  Non-Null Count   Dtype  \n",
                        "---  ------  --------------   -----  \n",
                        " 0   Date    108035 non-null  object \n",
                        " 1   PM2.5   86410 non-null   float64\n",
                        " 2   PM10    65329 non-null   float64\n",
                        " 3   NO2     91488 non-null   float64\n",
                        " 4   CO      95037 non-null   float64\n",
                        " 5   SO2     82831 non-null   float64\n",
                        " 6   O3      82467 non-null   float64\n",
                        " 7   AQI     87025 non-null   float64\n",
                        "dtypes: float64(7), object(1)\n",
                        "memory usage: 6.6+ MB\n"
                    ]
                }
            ],
            "source": [
                "# Descartamos las variables que no serán usadas, dejando las string para su posterior union\n",
                "estacion_dia_filtrado_string = estacion_dia[['Date', 'PM2.5', 'PM10', 'NO2', 'CO', 'SO2', 'O3', 'AQI']]\n",
                "estacion_dia_filtrado_string.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0ffb203d",
            "metadata": {},
            "source": [
                "<h3 style=\"text-align: left; color: red;\">Datos por Estación-hora y filtrado</h3>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 181,
            "id": "7fd994a9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2589083 entries, 0 to 2589082\n",
                        "Data columns (total 16 columns):\n",
                        " #   Column      Dtype  \n",
                        "---  ------      -----  \n",
                        " 0   StationId   object \n",
                        " 1   Datetime    object \n",
                        " 2   PM2.5       float64\n",
                        " 3   PM10        float64\n",
                        " 4   NO          float64\n",
                        " 5   NO2         float64\n",
                        " 6   NOx         float64\n",
                        " 7   NH3         float64\n",
                        " 8   CO          float64\n",
                        " 9   SO2         float64\n",
                        " 10  O3          float64\n",
                        " 11  Benzene     float64\n",
                        " 12  Toluene     float64\n",
                        " 13  Xylene      float64\n",
                        " 14  AQI         float64\n",
                        " 15  AQI_Bucket  object \n",
                        "dtypes: float64(13), object(3)\n",
                        "memory usage: 316.1+ MB\n"
                    ]
                }
            ],
            "source": [
                "estacion_hora.describe()\n",
                "estacion_hora.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 182,
            "id": "e9f0b2ce",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2589083 entries, 0 to 2589082\n",
                        "Data columns (total 8 columns):\n",
                        " #   Column    Dtype  \n",
                        "---  ------    -----  \n",
                        " 0   Datetime  object \n",
                        " 1   PM2.5     float64\n",
                        " 2   PM10      float64\n",
                        " 3   NO2       float64\n",
                        " 4   CO        float64\n",
                        " 5   SO2       float64\n",
                        " 6   O3        float64\n",
                        " 7   AQI       float64\n",
                        "dtypes: float64(7), object(1)\n",
                        "memory usage: 158.0+ MB\n"
                    ]
                }
            ],
            "source": [
                "# Descartamos las variables que no serán usadas, dejando las string para su posterior union\n",
                "estacion_hora_filtrado_string = estacion_hora[['Datetime', 'PM2.5', 'PM10', 'NO2', 'CO', 'SO2', 'O3', 'AQI']]\n",
                "estacion_hora_filtrado_string.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "503d256d",
            "metadata": {},
            "source": [
                "<h3 style=\"text-align: left; color: red;\">Datos de Estaciones</h3>\n",
                "\n",
                "Este archivo al solo contener un id y campos de tipo objeto/string, no será usado directamente para la imputación. Tiene una función importante en la unión de \"tablas\" por su campo \"StationId\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 183,
            "id": "fefa3b9f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 230 entries, 0 to 229\n",
                        "Data columns (total 5 columns):\n",
                        " #   Column       Non-Null Count  Dtype \n",
                        "---  ------       --------------  ----- \n",
                        " 0   StationId    230 non-null    object\n",
                        " 1   StationName  230 non-null    object\n",
                        " 2   City         230 non-null    object\n",
                        " 3   State        230 non-null    object\n",
                        " 4   Status       133 non-null    object\n",
                        "dtypes: object(5)\n",
                        "memory usage: 9.1+ KB\n"
                    ]
                }
            ],
            "source": [
                "estaciones.describe()\n",
                "estaciones.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "38d1d4f1",
            "metadata": {},
            "source": [
                "<h3 style=\"text-align: left; color: red;\">Función para contar los valores NaN totales</h3>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 184,
            "id": "dc663718",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Valores NaN del csv ciudad-hora:  366370\n",
                        "Valores NaN del csv ciudad-dia:  13521\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_13804\\2595521705.py:3: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                        "  archivo = pd.read_csv(nombre_archivo_csv)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Valores NaN del csv estacion-hora:  1554807\n",
                        "Valores NaN del csv estacion-dia:  58038\n"
                    ]
                }
            ],
            "source": [
                "# Definimos una funcion para verificar los valores NaN que tenemos en cada fila, si al menos existe 1 valor NaN en la fila, toda la fila se declara como tal\n",
                "def contar_filas_con_nan_en_columnas_especificas(nombre_archivo_csv):\n",
                "    archivo = pd.read_csv(nombre_archivo_csv)\n",
                "    \n",
                "    variables = ['PM2.5', 'PM10', 'NO2', 'CO', 'SO2', 'O3', 'AQI']\n",
                "    archivo_filtrado = archivo[variables]\n",
                "    \n",
                "    filas_con_nan = archivo_filtrado.isna().any(axis = 1) # axis = 1 ya que revisamos por fila, operando a lo largo de las columnas\n",
                "    cantidad_filas_con_nan = filas_con_nan.sum() # Sumamos las filas que contienen al menos un campo en NaN\n",
                "    \n",
                "    return cantidad_filas_con_nan\n",
                "\n",
                "print(\"Valores NaN del csv ciudad-hora: \",contar_filas_con_nan_en_columnas_especificas(\"Archivos no imputados/city_hour.csv\"))\n",
                "print(\"Valores NaN del csv ciudad-dia: \",contar_filas_con_nan_en_columnas_especificas(\"Archivos no imputados/city_day.csv\"))\n",
                "print(\"Valores NaN del csv estacion-hora: \",contar_filas_con_nan_en_columnas_especificas(\"Archivos no imputados/station_hour.csv\"))\n",
                "print(\"Valores NaN del csv estacion-dia: \",contar_filas_con_nan_en_columnas_especificas(\"Archivos no imputados/station_day.csv\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e7756dda",
            "metadata": {},
            "source": [
                "<h4 style=\"text-align: left; color: red;\">Imputación mediante SimpleImputer, KNN y PCA en paralelo</h4>\n",
                "\n",
                "A continuación, definimos las funciones que conforman la función principal para imputar nuestros valores vacíos mediante un barrido rápido con SimpleImputer y un posterior complemento con KNN y PCA. Esto en división por \"chunks\" o bloques para ejecutar los procesos en paralelo y optimizar los recursos limitados del dispositivo. "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34492cbc",
            "metadata": {},
            "source": [
                "<h5 style=\"text-align: left; color: red;\">Función de barrido inicial con SimpleImputer</h5>\n",
                "\n",
                "Para que el barrido sea rápido, se utilizo el parámetro \"mean\" de SimpleImputer, que toma la media de la columna para reemplazar los valores faltantes. Todo esto preservando las columnas e índices del archivo original."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 185,
            "id": "2e2614c4",
            "metadata": {},
            "outputs": [],
            "source": [
                "def barrido_inicial(archivo_filtrado_pandas):\n",
                "    \n",
                "    simple_imputer = SimpleImputer(strategy = \"mean\") # Estrategia mean utilizada para optimizar\n",
                "    archivo_pandas_barrido_inicial = pd.DataFrame(simple_imputer.fit_transform(archivo_filtrado_pandas),\n",
                "                                     columns = archivo_filtrado_pandas.columns, # se guardan las columnas originales del archivo pandas para preservar el orden\n",
                "                                     index   = archivo_filtrado_pandas.index) # se guardan los indices originales del archivo pandas para preservar el orden\n",
                "    \n",
                "    return archivo_pandas_barrido_inicial"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1124540e",
            "metadata": {},
            "source": [
                "<h5 style=\"text-align: left; color: red;\">Función de complementación con PCA y KNN al barrido inicial</h5>\n",
                "\n",
                "Esta función (parte de la función principal) procesa un único chunk del DataFrame que se tiene como argumento mediante KNN y PCA, todo esto guardando en un archivo parquet (binario columnar) para que el procesamiento sea más rápido."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 186,
            "id": "f0642252",
            "metadata": {},
            "outputs": [],
            "source": [
                "def complemento_knn_PCA(chunk, iterador, carpeta_guardado, nombre_csv, n_pca = 5):\n",
                "    # chunk : El fragmento del DataFrame completo que se ejecuta\n",
                "    # iterador: En la función principal al utilizar un bucle for, este funciona como el contador del chunk actual\n",
                "    # carpeta_guardado : Es la carpeta donde se guardan los archivos .parquet obtenidos\n",
                "    # nombre_csv : Es el nombre base pasado como argumento para mejorar la comprensión del código y los archivos generados\n",
                "    # n_pca : Es el numero de componentes (variables creadas por PCA a partir de las originales) para aplicar PCA\n",
                "\n",
                "    print(f\"Chunk {iterador}: Imputando con PCA + KNN...\")\n",
                "\n",
                "    scaler = StandardScaler() # Aqui aplicamos la normalizacion de datos (que todas las variables se encuentren en la misma escala)     \n",
                "    original_index = chunk.index # Guardamos el indice original para preservar el orden al inicio de la division en chunks\n",
                "\n",
                "    scaled = scaler.fit_transform(chunk) # Estandariza (calcula media y desviacion estandar y las aplica) para mantener una misma escala\n",
                "\n",
                "    pca = PCA(n_components = n_pca) # Aplicamos PCA para reducir la dimensionalidad a 5 componentes principales\n",
                "    reduced = pca.fit_transform(scaled) # Estandarizamos lo hecho por PCA\n",
                "\n",
                "    knn = KNNImputer(n_neighbors = 3, weights='distance') # Aplicamos KNN en el chunk con los 3 registros mas similares, teniendo en cuenta que los vecinos mas cercanos tienen mas peso por \"distance\"\n",
                "    reduced_imputed = knn.fit_transform(reduced) # Imputamos con KNN en el espacio reducido dado por PCA\n",
                "\n",
                "    reconstructed = pca.inverse_transform(reduced_imputed) # Dado que reducimos la dimensionalidad a 5 anteriormente, reconvertimos los datos desde el espacio reducido\n",
                "\n",
                "    # Revertimos la estandarización realizada con StandardScaler()\n",
                "    final_chunk = pd.DataFrame(scaler.inverse_transform(reconstructed), # Los datos ya imputados con SimpleImputer, KNN y PCA\n",
                "                               columns = chunk.columns, # verificamos los mismos nombres de columnas que el original\n",
                "                               index   = original_index) # restauramos los indices originales del chunk, para no perder el orden\n",
                "\n",
                "    nombre_archivo = os.path.splitext(os.path.basename(nombre_csv))[0] # Extraemos el nombre del archivo desde la ruta completa, separando el nombre del archivo de la extensión, usando [0] para tomar solo el nombre de la tupla (\"nombre\", \".extension\")\n",
                "    os.makedirs(carpeta_guardado, exist_ok = True) # Creamos la carpeta solo si no existe, si ya existe, no se crea nada\n",
                "    os.makedirs(\"Parquets\", exist_ok = True) # Creamos la carpeta Parquets con el mismo criterio anterior\n",
                "    output_file = os.path.join(carpeta_guardado, f\"{nombre_archivo}_chunk_{iterador}.parquet\") # Definimos la ruta donde se van a guardar los archivos .parquet mediante el nombre antes extraído.\n",
                "    final_chunk.to_parquet(output_file) # Guardamos el dataframe en formato parquet hacia la ruta definida\n",
                "    print(f\"Chunk {iterador} guardado en {output_file}.\") \n",
                "\n",
                "    return output_file"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f921e2d2",
            "metadata": {},
            "source": [
                "<h5 style=\"text-align: left; color: red;\">Función principal para la imputación de archivos</h5>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 187,
            "id": "03488481",
            "metadata": {},
            "outputs": [],
            "source": [
                "def imputacion_knn_pca_simpleImputer(archivo_pandas, nombre_csv): \n",
                "\n",
                "    # archivo_pandas: El DataFrame leido al inicio a partir del csv\n",
                "    # nombre_csv: Es el nombre base pasado como argumento para mejorar la comprensión del código y los archivos generados\n",
                "\n",
                "    indice_original = archivo_pandas.index # Almacenamos el índice completo del DataFrame para mantener el orden al final del proceso.\n",
                "\n",
                "    n_chunks = 10  # Definimos la cantidad de chunks en las que será dividido el DataFrame\n",
                "    chunks   = np.array_split(barrido_inicial(archivo_pandas), n_chunks) # Llamamos a barrido_inicial() para obtener los valores faltantes rellenados solo con la media por SimpleImputer(), dividiendo este en 10 trozos los mas iguales posible\n",
                "    \n",
                "    nombre_archivo   = os.path.splitext(os.path.basename(nombre_csv))[0] # Extraemos el nombre del archivo a partir de la ruta completa\n",
                "    carpeta_guardado = os.path.join(\"Parquets\", f\"parquets_{nombre_archivo}\") # Definimos la ruta donde se van a guardar los archivos .parquet\n",
                "    os.makedirs(carpeta_guardado, exist_ok = True) # Creamos una carpeta dentro de Parquets solo si no existe, donde se van a guardar los chunks en orden\n",
                "\n",
                "    numero_procesos = min(8, os.cpu_count()) # Definimos el máximo de procesos a realizarse, pueden ser tantos núcleos como tenga el dispositivo o 8 si tiene más.\n",
                "    result = Parallel(n_jobs = numero_procesos)( # Con Parallel realizamos KNN y PCA en cada chunk, creando un entorno para ejecutarlos en paralelo\n",
                "        delayed(complemento_knn_PCA)(chunk, iterador, carpeta_guardado, nombre_csv, n_pca = 5) for iterador, chunk in enumerate(chunks) # delayed (decorador) programa la ejecucion de nuestra función, ya que Parallel recibe una \"lista\" a realizarse, pasando esta \"lista\" mediante el bucle for.\n",
                "    )\n",
                "\n",
                "    print(\"Archivos resultantes: \\n\", result)\n",
                "    lista_dataframes = [pd.read_parquet(f) for f in sorted(glob.glob(os.path.join(carpeta_guardado, f\"{nombre_archivo}_chunk_*.parquet\")))] # Generamos una ruta con el patrón definido para el archivo .parquet, donde mediante glob() y el bucle for con sorted ordenamos en orden los archivos buscados en la carpeta. Todo esto para leerlos y pasarlos a DataFrame, obteniendo una lista de n Dataframes. (n es el numero de chunks)\n",
                "    \n",
                "    dataframe_final = pd.concat(lista_dataframes).sort_index() #  Mediante concat() unimos los DataFrames de la lista obtenida anteriormente, ordenando las filas por su índice original para mantener el orden.\n",
                "    \n",
                "    rangos_variables = archivo_pandas.agg(['min', 'max']) # Aqui definimos los rangos maximos de cada variable, para evitar valores negativos o muy inusuales\n",
                "\n",
                "    # Eliminamos mediante un bucle para corregir los outliers negativos o excesivamente fuera de lo normal que aparecen por la aplicacion seguida de PCA y KNN en baja dimension\n",
                "    for columna in rangos_variables.columns:\n",
                "        min_real = rangos_variables.loc['min', columna]\n",
                "        max_real = rangos_variables.loc['max', columna]\n",
                "        dataframe_final[columna] = dataframe_final[columna].clip(lower = min_real, upper = max_real)\n",
                "        \n",
                "    print(\"Imputacion terminada\")\n",
                "\n",
                "    # Verificador para ver si todavía existe algun valor NaN\n",
                "    if dataframe_final.isnull().values.any() :\n",
                "        print(\"Quedan valores vacios en el archivo.\")\n",
                "    else:\n",
                "        print(\"No quedan valores vacios en el archivo\")\n",
                "\n",
                "    # Verificador para ver si se mantuvieron los indices comparando con su original\n",
                "    if not (dataframe_final.index == indice_original).all():\n",
                "        print(\"Los índices se desordenaron\")\n",
                "    else:\n",
                "        print(\"Indices iguales.\")\n",
                "    \n",
                "    archivo_csv = f\"{nombre_archivo}_imputado\" # Definimos el nombre del archivo csv a guardar\n",
                "    os.makedirs(\"Archivos Imputados\", exist_ok = True) # Creamos la carpeta donde se guardaran los archivos imputados, solo si existe\n",
                "    carpeta_guardar_imputados = os.path.join(\"Archivos Imputados\", f\"{archivo_csv}.csv\") # Definimos la ruta donde se guardaran, que es dentro de la carpeta antes creada\n",
                "    \n",
                "    dataframe_final.to_csv(carpeta_guardar_imputados, index=True) # Transformamos el archivo pandas a csv, donde este se guardara en la carpeta antes definida\n",
                "\n",
                "    return None"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "47ae9325",
            "metadata": {},
            "source": [
                "<h4 style=\"text-align: left; color: red;\"><i>Imputación Ciudad-Hora</i></h4>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 188,
            "id": "385fa0a3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
                        "  return bound(*args, **kwds)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Archivos resultantes: \n",
                        " ['Parquets\\\\parquets_city_hour\\\\city_hour_chunk_0.parquet', 'Parquets\\\\parquets_city_hour\\\\city_hour_chunk_1.parquet', 'Parquets\\\\parquets_city_hour\\\\city_hour_chunk_2.parquet', 'Parquets\\\\parquets_city_hour\\\\city_hour_chunk_3.parquet', 'Parquets\\\\parquets_city_hour\\\\city_hour_chunk_4.parquet', 'Parquets\\\\parquets_city_hour\\\\city_hour_chunk_5.parquet', 'Parquets\\\\parquets_city_hour\\\\city_hour_chunk_6.parquet', 'Parquets\\\\parquets_city_hour\\\\city_hour_chunk_7.parquet', 'Parquets\\\\parquets_city_hour\\\\city_hour_chunk_8.parquet', 'Parquets\\\\parquets_city_hour\\\\city_hour_chunk_9.parquet']\n",
                        "Imputacion terminada\n",
                        "No quedan valores vacios en el archivo\n",
                        "Indices iguales.\n"
                    ]
                }
            ],
            "source": [
                "ciudad_hora_filtrado_2 = ciudad_hora[['PM2.5', 'PM10', 'NO2', 'CO', 'SO2', 'O3', 'AQI']]\n",
                "imputacion_knn_pca_simpleImputer(ciudad_hora_filtrado_2, \"city_hour\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a6370e90",
            "metadata": {},
            "source": [
                "<h4 style=\"text-align: left; color: red;\"><i>Imputación Ciudad-Día</i></h4>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 189,
            "id": "ced97e92",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
                        "  return bound(*args, **kwds)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Archivos resultantes: \n",
                        " ['Parquets\\\\parquets_city_day\\\\city_day_chunk_0.parquet', 'Parquets\\\\parquets_city_day\\\\city_day_chunk_1.parquet', 'Parquets\\\\parquets_city_day\\\\city_day_chunk_2.parquet', 'Parquets\\\\parquets_city_day\\\\city_day_chunk_3.parquet', 'Parquets\\\\parquets_city_day\\\\city_day_chunk_4.parquet', 'Parquets\\\\parquets_city_day\\\\city_day_chunk_5.parquet', 'Parquets\\\\parquets_city_day\\\\city_day_chunk_6.parquet', 'Parquets\\\\parquets_city_day\\\\city_day_chunk_7.parquet', 'Parquets\\\\parquets_city_day\\\\city_day_chunk_8.parquet', 'Parquets\\\\parquets_city_day\\\\city_day_chunk_9.parquet']\n",
                        "Imputacion terminada\n",
                        "No quedan valores vacios en el archivo\n",
                        "Indices iguales.\n"
                    ]
                }
            ],
            "source": [
                "ciudad_dia_filtrado_2 = ciudad_dia[['PM2.5', 'PM10', 'NO2', 'CO', 'SO2', 'O3', 'AQI']]\n",
                "imputacion_knn_pca_simpleImputer(ciudad_dia_filtrado_2, \"city_day\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "533116a7",
            "metadata": {},
            "source": [
                "<h4 style=\"text-align: left; color: red;\"><i>Imputación Estación-Hora</i></h4>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 190,
            "id": "8cb895f5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2589083 entries, 0 to 2589082\n",
                        "Data columns (total 7 columns):\n",
                        " #   Column  Dtype  \n",
                        "---  ------  -----  \n",
                        " 0   PM2.5   float64\n",
                        " 1   PM10    float64\n",
                        " 2   NO2     float64\n",
                        " 3   CO      float64\n",
                        " 4   SO2     float64\n",
                        " 5   O3      float64\n",
                        " 6   AQI     float64\n",
                        "dtypes: float64(7)\n",
                        "memory usage: 138.3 MB\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
                        "  return bound(*args, **kwds)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Archivos resultantes: \n",
                        " ['Parquets\\\\parquets_station_hour\\\\station_hour_chunk_0.parquet', 'Parquets\\\\parquets_station_hour\\\\station_hour_chunk_1.parquet', 'Parquets\\\\parquets_station_hour\\\\station_hour_chunk_2.parquet', 'Parquets\\\\parquets_station_hour\\\\station_hour_chunk_3.parquet', 'Parquets\\\\parquets_station_hour\\\\station_hour_chunk_4.parquet', 'Parquets\\\\parquets_station_hour\\\\station_hour_chunk_5.parquet', 'Parquets\\\\parquets_station_hour\\\\station_hour_chunk_6.parquet', 'Parquets\\\\parquets_station_hour\\\\station_hour_chunk_7.parquet', 'Parquets\\\\parquets_station_hour\\\\station_hour_chunk_8.parquet', 'Parquets\\\\parquets_station_hour\\\\station_hour_chunk_9.parquet']\n",
                        "Imputacion terminada\n",
                        "No quedan valores vacios en el archivo\n",
                        "Indices iguales.\n"
                    ]
                }
            ],
            "source": [
                "estacion_hora_union = pd.merge(estacion_hora, estaciones, on = \"StationId\", how = \"inner\")\n",
                "estacion_hora_filtrado = estacion_hora_union[['PM2.5', 'PM10', 'NO2', 'CO', 'SO2', 'O3', 'AQI']]\n",
                "estacion_hora_filtrado.info()\n",
                "imputacion_knn_pca_simpleImputer(estacion_hora_filtrado, \"station_hour\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ec979db3",
            "metadata": {},
            "source": [
                "<h4 style=\"text-align: left; color: red;\"><i>Imputación Estación-Día</i></h4>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 191,
            "id": "aaf97874",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 108035 entries, 0 to 108034\n",
                        "Data columns (total 7 columns):\n",
                        " #   Column  Non-Null Count  Dtype  \n",
                        "---  ------  --------------  -----  \n",
                        " 0   PM2.5   86410 non-null  float64\n",
                        " 1   PM10    65329 non-null  float64\n",
                        " 2   NO2     91488 non-null  float64\n",
                        " 3   CO      95037 non-null  float64\n",
                        " 4   SO2     82831 non-null  float64\n",
                        " 5   O3      82467 non-null  float64\n",
                        " 6   AQI     87025 non-null  float64\n",
                        "dtypes: float64(7)\n",
                        "memory usage: 5.8 MB\n",
                        "Archivos resultantes: \n",
                        " ['Parquets\\\\parquets_station_day\\\\station_day_chunk_0.parquet', 'Parquets\\\\parquets_station_day\\\\station_day_chunk_1.parquet', 'Parquets\\\\parquets_station_day\\\\station_day_chunk_2.parquet', 'Parquets\\\\parquets_station_day\\\\station_day_chunk_3.parquet', 'Parquets\\\\parquets_station_day\\\\station_day_chunk_4.parquet', 'Parquets\\\\parquets_station_day\\\\station_day_chunk_5.parquet', 'Parquets\\\\parquets_station_day\\\\station_day_chunk_6.parquet', 'Parquets\\\\parquets_station_day\\\\station_day_chunk_7.parquet', 'Parquets\\\\parquets_station_day\\\\station_day_chunk_8.parquet', 'Parquets\\\\parquets_station_day\\\\station_day_chunk_9.parquet']\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
                        "  return bound(*args, **kwds)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Imputacion terminada\n",
                        "No quedan valores vacios en el archivo\n",
                        "Indices iguales.\n"
                    ]
                }
            ],
            "source": [
                "estacion_dia_union = pd.merge(estacion_dia, estaciones, on = \"StationId\", how = \"inner\")\n",
                "estacion_dia_filtrado = estacion_dia_union[['PM2.5', 'PM10', 'NO2', 'CO', 'SO2', 'O3', 'AQI']]\n",
                "estacion_dia_filtrado.info()\n",
                "imputacion_knn_pca_simpleImputer(estacion_dia_filtrado, \"station_day\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
